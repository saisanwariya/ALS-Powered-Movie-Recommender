{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">ALS-Powered Movie Recommender: Sparking Personalized Cinema Selections</h1>\n",
    "\n",
    "## Sai Sanwariya Narayan\n",
    "\n",
    "This project focuses on developing a sophisticated movie recommendation system utilizing the Alternating Least Squares (ALS) algorithm within the Apache Spark framework:\n",
    "\n",
    "1. **Implementing ALS for Movie Recommendations**: based on user reviews, leveraging the MLlib library in Spark.\n",
    "\n",
    "2. **Data Management and Analysis**: Skills in handling data through splitting it into training, validation, and testing sets. This process is crucial for building robust machine learning models.\n",
    "\n",
    "3. **Error Calculation and Hyperparameter Tuning**: Techniques to calculate training, validation, and testing errors, and understanding the importance of tuning hyperparameters for optimal model performance.\n",
    "\n",
    "4. **Efficiency in Processing**: Utilizing Spark's RDD transformations and CheckPoint features to enhance processing efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will need to `pip install pandas` in the terminal for this within the your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"Lab6 ALS-based Recommendation Systems\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_schema = StructType([ StructField(\"UserID\", IntegerType(), False ), \\\n",
    "                            StructField(\"MovieID\", IntegerType(), True), \\\n",
    "                            StructField(\"Rating\", FloatType(), True ), \\\n",
    "                            StructField(\"RatingID\", IntegerType(), True ), \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_DF = ss.read.csv(\"/storage/home/ratings_2.csv\", schema=rating_schema, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: float (nullable = true)\n",
      " |-- RatingID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = ratings_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UserID=1, MovieID=31, Rating=2.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_DF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserID=1, MovieID=31, Rating=2.5),\n",
       " Row(UserID=1, MovieID=1029, Rating=3.0),\n",
       " Row(UserID=1, MovieID=1061, Rating=3.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_RDD = ratings2_DF.rdd\n",
    "ratings2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting Data into Three Sets: Training Data, Evaluation Data, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([3,1,1], 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input (UserID, MovieID) for training, validation and for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie Recommendation Model \n",
    "## using ALS (from `PySpark.MLlib.recommendation` module) and training data. Choose a rank between 3 and 6, a randon number for the seed, 30 iterations, 0.1 regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS.train(training_RDD, 4, seed=17, iterations=30, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Training Error of the ALS recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction_RDD = model.predictAll(training_input_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=253, product=37739, rating=3.4122745854752115),\n",
       " Rating(user=402, product=69069, rating=3.9043121138075443),\n",
       " Rating(user=213, product=44828, rating=0.4832854926762248),\n",
       " Rating(user=428, product=5618, rating=4.406297513023318)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction_RDD.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Ways to Access Elements of a 'Row' object in an RDD. \n",
    "We are going to demonstrate these three methods for accessing/transforming the format of the RDD so that ``(<user>, <movie>)`` is in the key position so that we can join the RDD containing the actual rating with the RDD containing the predicted rating (for calculating prediction errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserID=1, MovieID=31, Rating=2.5),\n",
       " Row(UserID=1, MovieID=1263, Rating=2.0),\n",
       " Row(UserID=1, MovieID=1343, Rating=2.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Access elements of a row using column name of the DataFrame (from which the RDD came from) using the syntax ``<row variable>[ <ColumnName> ]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output_RDD = training_RDD.map(lambda x: ( (x['UserID'], x['MovieID']), x['Rating'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Access elements of a row using column name (that does not contain space) of the DataFrame schema (from which the RDD came from) using the syntax ``<row variable>.<ColumnName>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output2_RDD = training_RDD.map(lambda x: ( ( x.UserID, x.MovieID ), x.Rating ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Access elements of a row using column name of the DataFrame (from which the RDD came from) using the syntax ``<row variable>[<index>]`` where ``<index>`` is the integer that indicates the position of the element in the row (starting with 0 for the first element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_output3_RDD = training_RDD.map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), 2.5), ((1, 1263), 2.0), ((1, 1343), 2.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_target_output3_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the model output of training data into the format of `( (<UserID> <MovieID>), <predictedRating> )` so that we can later join it with training target outpt RDD for computing Root Mean Square Error of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction2_RDD = training_prediction_RDD.map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((253, 37739), 3.4122745854752115),\n",
       " ((402, 69069), 3.9043121138075443),\n",
       " ((213, 44828), 0.4832854926762248)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_evaluation_RDD = training_target_output_RDD.join(training_prediction2_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), (2.5, 2.57288711148178)),\n",
       " ((1, 1263), (2.0, 2.2544347728100416)),\n",
       " ((1, 1343), (2.0, 2.185310938803255))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6486012428483423\n"
     ]
    }
   ],
   "source": [
    "print(training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Validation Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction_RDD = model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((48, 44828), 0.40229038956148866),\n",
       " ((331, 5618), 4.31991825689105),\n",
       " ((577, 5618), 4.9267050424199645)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining `validation_RDD` (after transforming it into the same key value pair format, and `validation_prediction_RDD` to prepare for RMS error calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation_RDD = validation_RDD.map(lambda y: ((y[0], y[1]), y[2] ) ).join(validation_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1129), (2.0, 2.0247078728191674)),\n",
       " ((1, 1287), (2.0, 2.2419968835570394)),\n",
       " ((2, 52), (3.0, 3.5191740150846336))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating RMS error for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2 ).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393572719475977\n"
     ]
    }
   ],
   "source": [
    "print(validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "## Iterating through all possible combination of a set of values for three hyperparameters for ALS Recommendation Model:\n",
    "- rank (k)\n",
    "- regularization\n",
    "- iterations \n",
    "## Each hyperparameter value combination is used to construct an ALS recommendation model using training data, but evaluate using Evaluation Data\n",
    "## The evaluation results are saved in a Pandas DataFrame \n",
    "``\n",
    "hyperparams_eval_df\n",
    "``\n",
    "## The best hyperprameter value combination is stored in 4 variables\n",
    "``\n",
    "best_k, best_regularization, best_iterations, and lowest_validation_error\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting of hyperparameters (rank k, regularization parameter, and number of iterations) to create and evaluate ALS recommendation models to find the best model among all those created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best rank k is  10 , regularization =  0.2 , iterations =  15 . Validation Error = 0.9191682693047565\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [15, 30]\n",
    "regularization_list = [0.1, 0.2, 0.3]\n",
    "rank_list = [4, 7, 10, 13]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2])   )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( ( y[0], y[1]), y[2] ) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, validation_error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if validation_error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index - 1\n",
    "                lowest_validation_error = validation_error\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Testing Data to Evaluate the Model built using the Best Hyperparameters                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the best hyperparameter combination using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing Error for rank k = 10  regularization =  0.2 , iterations =  15  is :  0.9223692317363631\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "model = ALS.train(training_RDD, best_k, seed=seed, iterations=best_iterations, lambda_=best_regularization)\n",
    "testing_prediction_RDD=model.predictAll(testing_input_RDD).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "testing_evaluation_RDD= test_RDD.map(lambda x: ((x[0], x[1]), x[2])).join(testing_prediction_RDD)\n",
    "testing_error = math.sqrt(testing_evaluation_RDD.map(lambda x: (x[1][0]-x[1][1])**2).mean())\n",
    "print('The Testing Error for rank k =', best_k, ' regularization = ', best_regularization, ', iterations = ', \\\n",
    "      best_iterations, ' is : ', testing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_k, best_regularization, best_iterations, lowest_validation_error, testing_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"k\", FloatType(), True), \\\n",
    "                      StructField(\"regularization\", FloatType(), True ), \\\n",
    "                      StructField(\"iterations\", FloatType(), True), \\\n",
    "                      StructField(\"Validation RMS\", FloatType(), True), \\\n",
    "                      StructField(\"Testing RMS\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to a Spark DataFrame, so that it can be written in the cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_RMS_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/ALSHyperParamsTuning\"\n",
    "HyperParams_RMS_DF.write.option(\"header\", True).csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
